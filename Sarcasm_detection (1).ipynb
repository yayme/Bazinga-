{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "kYcuczd5y3OK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.optim import Adam\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading dataset\n",
        "dataset= load_dataset(\"tweet_eval\", \"irony\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained ('bert-base-uncased')\n",
        "\n",
        "class SarcasmDataset (torch.utils.data.Dataset):\n",
        "    def __init__ (self, split):\n",
        "        self.data=dataset[split]\n",
        "        self.texts = self.data[ 'text']\n",
        "        self.labels= self.data['label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text=self.texts[idx]\n",
        "        label=self.labels[idx]\n",
        "        encoding=tokenizer(text, truncation=True, padding ='max_length', max_length=64, return_tensors='pt')\n",
        "\n",
        "        return {key: val.squeeze(0) for key,val in encoding.items()}, torch.tensor(label)\n",
        "\n",
        "\n",
        "train_data=SarcasmDataset('train')\n",
        "val_data= SarcasmDataset('validation')\n",
        "train_loader = DataLoader(train_data, batch_size=16 , shuffle =True)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sypgpARkzE6_"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(val_data), type(train_data), type(train_loader), len(train_loader)"
      ],
      "metadata": {
        "id": "X0LdoBeN2XT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad880a8-4993-4b0c-f3ed-bdf7467e89d9"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2862,\n",
              " 955,\n",
              " __main__.SarcasmDataset,\n",
              " torch.utils.data.dataloader.DataLoader,\n",
              " 179)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SarcasmClassifier (nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert =BertModel.from_pretrained ('bert-base-uncased')\n",
        "        self.classifier=nn.Linear(self.bert.config.hidden_size, 1)\n",
        "        self.sigmoid=nn.Sigmoid()\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs= self.bert(input_ids= input_ids, attention_mask=attention_mask)\n",
        "        pool_output=outputs.pooler_output\n",
        "        logits=self.classifier(pool_output)\n",
        "        return self.sigmoid(logits).squeeze(-1)\n",
        "\n",
        "\n",
        "model= SarcasmClassifier()\n",
        "\n",
        "\n",
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr=2e-5)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xi3VMbrezTKO"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for epoch in range(3):\n",
        "    print(f\"Current epoch {epoch}\")\n",
        "\n",
        "    model.train()\n",
        "    total_loss=0\n",
        "    for batch, labels in train_loader:\n",
        "        input_ids=batch['input_ids' ].to(device)\n",
        "        attention_mask= batch['attention_mask'].to(device)\n",
        "        labels=labels.float().to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=model(input_ids, attention_mask)\n",
        "        loss=criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss+=loss.item()\n",
        "\n",
        "        print(f\"train loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total =0\n",
        "    with torch.no_grad():\n",
        "        for batch, labels in val_loader:\n",
        "            input_ids= batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels= labels.to(device)\n",
        "            outputs= model(input_ids, attention_mask)\n",
        "            preds=(outputs > 0.5).long()\n",
        "            total_correct+= (preds==labels).sum().item()\n",
        "            total+=labels.size(0)\n",
        "\n",
        "    print(f\"Validation accuracy: {total_correct/total: .4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6ps2dBjR24Uk",
        "outputId": "ff93e4f0-e3a5-4893-a380-11fc1188fc6c"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current epoch 0\n",
            "train loss: 0.0040\n",
            "train loss: 0.0077\n",
            "train loss: 0.0116\n",
            "train loss: 0.0155\n",
            "train loss: 0.0193\n",
            "train loss: 0.0230\n",
            "train loss: 0.0268\n",
            "train loss: 0.0307\n",
            "train loss: 0.0347\n",
            "train loss: 0.0382\n",
            "train loss: 0.0421\n",
            "train loss: 0.0462\n",
            "train loss: 0.0498\n",
            "train loss: 0.0537\n",
            "train loss: 0.0576\n",
            "train loss: 0.0614\n",
            "train loss: 0.0651\n",
            "train loss: 0.0689\n",
            "train loss: 0.0727\n",
            "train loss: 0.0764\n",
            "train loss: 0.0801\n",
            "train loss: 0.0840\n",
            "train loss: 0.0875\n",
            "train loss: 0.0914\n",
            "train loss: 0.0949\n",
            "train loss: 0.0989\n",
            "train loss: 0.1026\n",
            "train loss: 0.1063\n",
            "train loss: 0.1100\n",
            "train loss: 0.1135\n",
            "train loss: 0.1172\n",
            "train loss: 0.1207\n",
            "train loss: 0.1245\n",
            "train loss: 0.1282\n",
            "train loss: 0.1317\n",
            "train loss: 0.1355\n",
            "train loss: 0.1395\n",
            "train loss: 0.1433\n",
            "train loss: 0.1468\n",
            "train loss: 0.1502\n",
            "train loss: 0.1540\n",
            "train loss: 0.1574\n",
            "train loss: 0.1609\n",
            "train loss: 0.1647\n",
            "train loss: 0.1679\n",
            "train loss: 0.1715\n",
            "train loss: 0.1753\n",
            "train loss: 0.1791\n",
            "train loss: 0.1829\n",
            "train loss: 0.1865\n",
            "train loss: 0.1901\n",
            "train loss: 0.1931\n",
            "train loss: 0.1961\n",
            "train loss: 0.2001\n",
            "train loss: 0.2037\n",
            "train loss: 0.2071\n",
            "train loss: 0.2107\n",
            "train loss: 0.2146\n",
            "train loss: 0.2175\n",
            "train loss: 0.2207\n",
            "train loss: 0.2247\n",
            "train loss: 0.2283\n",
            "train loss: 0.2319\n",
            "train loss: 0.2363\n",
            "train loss: 0.2397\n",
            "train loss: 0.2430\n",
            "train loss: 0.2466\n",
            "train loss: 0.2501\n",
            "train loss: 0.2540\n",
            "train loss: 0.2575\n",
            "train loss: 0.2609\n",
            "train loss: 0.2643\n",
            "train loss: 0.2675\n",
            "train loss: 0.2710\n",
            "train loss: 0.2743\n",
            "train loss: 0.2780\n",
            "train loss: 0.2807\n",
            "train loss: 0.2838\n",
            "train loss: 0.2868\n",
            "train loss: 0.2898\n",
            "train loss: 0.2938\n",
            "train loss: 0.2977\n",
            "train loss: 0.3009\n",
            "train loss: 0.3044\n",
            "train loss: 0.3079\n",
            "train loss: 0.3114\n",
            "train loss: 0.3154\n",
            "train loss: 0.3182\n",
            "train loss: 0.3214\n",
            "train loss: 0.3253\n",
            "train loss: 0.3289\n",
            "train loss: 0.3326\n",
            "train loss: 0.3365\n",
            "train loss: 0.3398\n",
            "train loss: 0.3433\n",
            "train loss: 0.3468\n",
            "train loss: 0.3490\n",
            "train loss: 0.3520\n",
            "train loss: 0.3557\n",
            "train loss: 0.3596\n",
            "train loss: 0.3632\n",
            "train loss: 0.3666\n",
            "train loss: 0.3701\n",
            "train loss: 0.3740\n",
            "train loss: 0.3769\n",
            "train loss: 0.3801\n",
            "train loss: 0.3842\n",
            "train loss: 0.3876\n",
            "train loss: 0.3905\n",
            "train loss: 0.3937\n",
            "train loss: 0.3978\n",
            "train loss: 0.4017\n",
            "train loss: 0.4046\n",
            "train loss: 0.4083\n",
            "train loss: 0.4118\n",
            "train loss: 0.4154\n",
            "train loss: 0.4187\n",
            "train loss: 0.4223\n",
            "train loss: 0.4254\n",
            "train loss: 0.4296\n",
            "train loss: 0.4327\n",
            "train loss: 0.4357\n",
            "train loss: 0.4395\n",
            "train loss: 0.4430\n",
            "train loss: 0.4464\n",
            "train loss: 0.4497\n",
            "train loss: 0.4528\n",
            "train loss: 0.4562\n",
            "train loss: 0.4603\n",
            "train loss: 0.4644\n",
            "train loss: 0.4682\n",
            "train loss: 0.4716\n",
            "train loss: 0.4752\n",
            "train loss: 0.4792\n",
            "train loss: 0.4829\n",
            "train loss: 0.4855\n",
            "train loss: 0.4891\n",
            "train loss: 0.4933\n",
            "train loss: 0.4964\n",
            "train loss: 0.4999\n",
            "train loss: 0.5036\n",
            "train loss: 0.5065\n",
            "train loss: 0.5094\n",
            "train loss: 0.5132\n",
            "train loss: 0.5170\n",
            "train loss: 0.5204\n",
            "train loss: 0.5234\n",
            "train loss: 0.5267\n",
            "train loss: 0.5305\n",
            "train loss: 0.5349\n",
            "train loss: 0.5379\n",
            "train loss: 0.5417\n",
            "train loss: 0.5452\n",
            "train loss: 0.5483\n",
            "train loss: 0.5518\n",
            "train loss: 0.5555\n",
            "train loss: 0.5583\n",
            "train loss: 0.5618\n",
            "train loss: 0.5653\n",
            "train loss: 0.5685\n",
            "train loss: 0.5721\n",
            "train loss: 0.5757\n",
            "train loss: 0.5785\n",
            "train loss: 0.5825\n",
            "train loss: 0.5857\n",
            "train loss: 0.5891\n",
            "train loss: 0.5918\n",
            "train loss: 0.5957\n",
            "train loss: 0.6000\n",
            "train loss: 0.6031\n",
            "train loss: 0.6064\n",
            "train loss: 0.6099\n",
            "train loss: 0.6140\n",
            "train loss: 0.6176\n",
            "train loss: 0.6211\n",
            "train loss: 0.6247\n",
            "train loss: 0.6282\n",
            "train loss: 0.6319\n",
            "train loss: 0.6350\n",
            "Validation accuracy:  0.6869\n",
            "Current epoch 1\n",
            "train loss: 0.0035\n",
            "train loss: 0.0064\n",
            "train loss: 0.0088\n",
            "train loss: 0.0119\n",
            "train loss: 0.0149\n",
            "train loss: 0.0180\n",
            "train loss: 0.0207\n",
            "train loss: 0.0239\n",
            "train loss: 0.0266\n",
            "train loss: 0.0301\n",
            "train loss: 0.0334\n",
            "train loss: 0.0362\n",
            "train loss: 0.0395\n",
            "train loss: 0.0422\n",
            "train loss: 0.0454\n",
            "train loss: 0.0493\n",
            "train loss: 0.0521\n",
            "train loss: 0.0553\n",
            "train loss: 0.0579\n",
            "train loss: 0.0604\n",
            "train loss: 0.0637\n",
            "train loss: 0.0659\n",
            "train loss: 0.0688\n",
            "train loss: 0.0714\n",
            "train loss: 0.0754\n",
            "train loss: 0.0775\n",
            "train loss: 0.0806\n",
            "train loss: 0.0833\n",
            "train loss: 0.0874\n",
            "train loss: 0.0892\n",
            "train loss: 0.0930\n",
            "train loss: 0.0952\n",
            "train loss: 0.0987\n",
            "train loss: 0.1022\n",
            "train loss: 0.1057\n",
            "train loss: 0.1079\n",
            "train loss: 0.1099\n",
            "train loss: 0.1135\n",
            "train loss: 0.1164\n",
            "train loss: 0.1191\n",
            "train loss: 0.1215\n",
            "train loss: 0.1241\n",
            "train loss: 0.1263\n",
            "train loss: 0.1305\n",
            "train loss: 0.1328\n",
            "train loss: 0.1360\n",
            "train loss: 0.1386\n",
            "train loss: 0.1414\n",
            "train loss: 0.1430\n",
            "train loss: 0.1462\n",
            "train loss: 0.1492\n",
            "train loss: 0.1518\n",
            "train loss: 0.1546\n",
            "train loss: 0.1581\n",
            "train loss: 0.1604\n",
            "train loss: 0.1630\n",
            "train loss: 0.1655\n",
            "train loss: 0.1691\n",
            "train loss: 0.1712\n",
            "train loss: 0.1741\n",
            "train loss: 0.1783\n",
            "train loss: 0.1812\n",
            "train loss: 0.1829\n",
            "train loss: 0.1857\n",
            "train loss: 0.1878\n",
            "train loss: 0.1909\n",
            "train loss: 0.1938\n",
            "train loss: 0.1966\n",
            "train loss: 0.1980\n",
            "train loss: 0.2012\n",
            "train loss: 0.2039\n",
            "train loss: 0.2068\n",
            "train loss: 0.2086\n",
            "train loss: 0.2112\n",
            "train loss: 0.2131\n",
            "train loss: 0.2150\n",
            "train loss: 0.2181\n",
            "train loss: 0.2219\n",
            "train loss: 0.2243\n",
            "train loss: 0.2276\n",
            "train loss: 0.2303\n",
            "train loss: 0.2327\n",
            "train loss: 0.2342\n",
            "train loss: 0.2375\n",
            "train loss: 0.2404\n",
            "train loss: 0.2434\n",
            "train loss: 0.2464\n",
            "train loss: 0.2489\n",
            "train loss: 0.2530\n",
            "train loss: 0.2567\n",
            "train loss: 0.2592\n",
            "train loss: 0.2622\n",
            "train loss: 0.2647\n",
            "train loss: 0.2670\n",
            "train loss: 0.2694\n",
            "train loss: 0.2719\n",
            "train loss: 0.2747\n",
            "train loss: 0.2770\n",
            "train loss: 0.2806\n",
            "train loss: 0.2840\n",
            "train loss: 0.2869\n",
            "train loss: 0.2904\n",
            "train loss: 0.2928\n",
            "train loss: 0.2962\n",
            "train loss: 0.2996\n",
            "train loss: 0.3018\n",
            "train loss: 0.3049\n",
            "train loss: 0.3077\n",
            "train loss: 0.3100\n",
            "train loss: 0.3158\n",
            "train loss: 0.3188\n",
            "train loss: 0.3219\n",
            "train loss: 0.3247\n",
            "train loss: 0.3280\n",
            "train loss: 0.3304\n",
            "train loss: 0.3334\n",
            "train loss: 0.3363\n",
            "train loss: 0.3400\n",
            "train loss: 0.3428\n",
            "train loss: 0.3456\n",
            "train loss: 0.3480\n",
            "train loss: 0.3498\n",
            "train loss: 0.3527\n",
            "train loss: 0.3550\n",
            "train loss: 0.3575\n",
            "train loss: 0.3607\n",
            "train loss: 0.3623\n",
            "train loss: 0.3647\n",
            "train loss: 0.3679\n",
            "train loss: 0.3708\n",
            "train loss: 0.3734\n",
            "train loss: 0.3768\n",
            "train loss: 0.3784\n",
            "train loss: 0.3815\n",
            "train loss: 0.3840\n",
            "train loss: 0.3868\n",
            "train loss: 0.3892\n",
            "train loss: 0.3914\n",
            "train loss: 0.3936\n",
            "train loss: 0.3963\n",
            "train loss: 0.4006\n",
            "train loss: 0.4027\n",
            "train loss: 0.4046\n",
            "train loss: 0.4078\n",
            "train loss: 0.4109\n",
            "train loss: 0.4137\n",
            "train loss: 0.4158\n",
            "train loss: 0.4179\n",
            "train loss: 0.4208\n",
            "train loss: 0.4236\n",
            "train loss: 0.4256\n",
            "train loss: 0.4279\n",
            "train loss: 0.4307\n",
            "train loss: 0.4332\n",
            "train loss: 0.4350\n",
            "train loss: 0.4376\n",
            "train loss: 0.4416\n",
            "train loss: 0.4434\n",
            "train loss: 0.4465\n",
            "train loss: 0.4489\n",
            "train loss: 0.4525\n",
            "train loss: 0.4544\n",
            "train loss: 0.4565\n",
            "train loss: 0.4595\n",
            "train loss: 0.4625\n",
            "train loss: 0.4639\n",
            "train loss: 0.4651\n",
            "train loss: 0.4682\n",
            "train loss: 0.4713\n",
            "train loss: 0.4740\n",
            "train loss: 0.4778\n",
            "train loss: 0.4813\n",
            "train loss: 0.4836\n",
            "train loss: 0.4863\n",
            "train loss: 0.4884\n",
            "train loss: 0.4899\n",
            "train loss: 0.4942\n",
            "train loss: 0.4984\n",
            "train loss: 0.5011\n",
            "Validation accuracy:  0.7162\n",
            "Current epoch 2\n",
            "train loss: 0.0021\n",
            "train loss: 0.0034\n",
            "train loss: 0.0051\n",
            "train loss: 0.0074\n",
            "train loss: 0.0091\n",
            "train loss: 0.0118\n",
            "train loss: 0.0133\n",
            "train loss: 0.0155\n",
            "train loss: 0.0170\n",
            "train loss: 0.0183\n",
            "train loss: 0.0191\n",
            "train loss: 0.0203\n",
            "train loss: 0.0222\n",
            "train loss: 0.0237\n",
            "train loss: 0.0258\n",
            "train loss: 0.0280\n",
            "train loss: 0.0298\n",
            "train loss: 0.0305\n",
            "train loss: 0.0329\n",
            "train loss: 0.0338\n",
            "train loss: 0.0348\n",
            "train loss: 0.0367\n",
            "train loss: 0.0383\n",
            "train loss: 0.0401\n",
            "train loss: 0.0413\n",
            "train loss: 0.0420\n",
            "train loss: 0.0429\n",
            "train loss: 0.0460\n",
            "train loss: 0.0485\n",
            "train loss: 0.0503\n",
            "train loss: 0.0522\n",
            "train loss: 0.0542\n",
            "train loss: 0.0552\n",
            "train loss: 0.0580\n",
            "train loss: 0.0602\n",
            "train loss: 0.0628\n",
            "train loss: 0.0663\n",
            "train loss: 0.0678\n",
            "train loss: 0.0690\n",
            "train loss: 0.0711\n",
            "train loss: 0.0728\n",
            "train loss: 0.0751\n",
            "train loss: 0.0768\n",
            "train loss: 0.0794\n",
            "train loss: 0.0812\n",
            "train loss: 0.0832\n",
            "train loss: 0.0850\n",
            "train loss: 0.0869\n",
            "train loss: 0.0893\n",
            "train loss: 0.0911\n",
            "train loss: 0.0939\n",
            "train loss: 0.0977\n",
            "train loss: 0.0984\n",
            "train loss: 0.0997\n",
            "train loss: 0.1007\n",
            "train loss: 0.1024\n",
            "train loss: 0.1051\n",
            "train loss: 0.1064\n",
            "train loss: 0.1089\n",
            "train loss: 0.1109\n",
            "train loss: 0.1116\n",
            "train loss: 0.1128\n",
            "train loss: 0.1145\n",
            "train loss: 0.1156\n",
            "train loss: 0.1171\n",
            "train loss: 0.1186\n",
            "train loss: 0.1205\n",
            "train loss: 0.1218\n",
            "train loss: 0.1237\n",
            "train loss: 0.1249\n",
            "train loss: 0.1258\n",
            "train loss: 0.1270\n",
            "train loss: 0.1284\n",
            "train loss: 0.1292\n",
            "train loss: 0.1316\n",
            "train loss: 0.1322\n",
            "train loss: 0.1346\n",
            "train loss: 0.1363\n",
            "train loss: 0.1378\n",
            "train loss: 0.1384\n",
            "train loss: 0.1411\n",
            "train loss: 0.1436\n",
            "train loss: 0.1455\n",
            "train loss: 0.1463\n",
            "train loss: 0.1478\n",
            "train loss: 0.1505\n",
            "train loss: 0.1528\n",
            "train loss: 0.1532\n",
            "train loss: 0.1546\n",
            "train loss: 0.1566\n",
            "train loss: 0.1588\n",
            "train loss: 0.1609\n",
            "train loss: 0.1626\n",
            "train loss: 0.1634\n",
            "train loss: 0.1646\n",
            "train loss: 0.1663\n",
            "train loss: 0.1678\n",
            "train loss: 0.1696\n",
            "train loss: 0.1721\n",
            "train loss: 0.1727\n",
            "train loss: 0.1734\n",
            "train loss: 0.1752\n",
            "train loss: 0.1758\n",
            "train loss: 0.1786\n",
            "train loss: 0.1795\n",
            "train loss: 0.1800\n",
            "train loss: 0.1813\n",
            "train loss: 0.1823\n",
            "train loss: 0.1834\n",
            "train loss: 0.1853\n",
            "train loss: 0.1879\n",
            "train loss: 0.1884\n",
            "train loss: 0.1901\n",
            "train loss: 0.1926\n",
            "train loss: 0.1946\n",
            "train loss: 0.1963\n",
            "train loss: 0.1972\n",
            "train loss: 0.1996\n",
            "train loss: 0.2029\n",
            "train loss: 0.2041\n",
            "train loss: 0.2053\n",
            "train loss: 0.2065\n",
            "train loss: 0.2080\n",
            "train loss: 0.2105\n",
            "train loss: 0.2139\n",
            "train loss: 0.2156\n",
            "train loss: 0.2185\n",
            "train loss: 0.2195\n",
            "train loss: 0.2200\n",
            "train loss: 0.2213\n",
            "train loss: 0.2242\n",
            "train loss: 0.2258\n",
            "train loss: 0.2270\n",
            "train loss: 0.2286\n",
            "train loss: 0.2299\n",
            "train loss: 0.2307\n",
            "train loss: 0.2322\n",
            "train loss: 0.2339\n",
            "train loss: 0.2355\n",
            "train loss: 0.2381\n",
            "train loss: 0.2407\n",
            "train loss: 0.2422\n",
            "train loss: 0.2435\n",
            "train loss: 0.2452\n",
            "train loss: 0.2462\n",
            "train loss: 0.2473\n",
            "train loss: 0.2484\n",
            "train loss: 0.2510\n",
            "train loss: 0.2516\n",
            "train loss: 0.2526\n",
            "train loss: 0.2536\n",
            "train loss: 0.2553\n",
            "train loss: 0.2580\n",
            "train loss: 0.2601\n",
            "train loss: 0.2621\n",
            "train loss: 0.2638\n",
            "train loss: 0.2668\n",
            "train loss: 0.2683\n",
            "train loss: 0.2696\n",
            "train loss: 0.2715\n",
            "train loss: 0.2737\n",
            "train loss: 0.2747\n",
            "train loss: 0.2755\n",
            "train loss: 0.2791\n",
            "train loss: 0.2821\n",
            "train loss: 0.2857\n",
            "train loss: 0.2869\n",
            "train loss: 0.2892\n",
            "train loss: 0.2918\n",
            "train loss: 0.2943\n",
            "train loss: 0.2964\n",
            "train loss: 0.2978\n",
            "train loss: 0.2998\n",
            "train loss: 0.3017\n",
            "train loss: 0.3033\n",
            "train loss: 0.3054\n",
            "train loss: 0.3086\n",
            "train loss: 0.3108\n",
            "train loss: 0.3131\n",
            "Validation accuracy:  0.7089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"sarcasm_model.pt\")\n"
      ],
      "metadata": {
        "id": "jM3s-Byf3Ams"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_text(texts: str):\n",
        "  texts=[texts]\n",
        "  inputs=tokenizer(texts, padding=True, truncation=True, return_tensors='pt', return_token_type_ids=False)\n",
        "  inputs={k: v.to(device) for k,v in inputs.items()}\n",
        "\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  with torch.no_grad():\n",
        "    proba=model(**inputs)\n",
        "  preds=(proba > 0.5).long()\n",
        "\n",
        "  return [(text,pred.item(), prob.item()) for text, pred, prob in zip(texts,preds, proba)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z_uu1TMk4K_e"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def print_result(texts: str):\n",
        "  results= custom_text(texts)\n",
        "  for text, pred, prob in results:\n",
        "    label= \"Bazinga\" if pred==1 else \"Not Funny at all.\"\n",
        "    print(f'Text: {text} \\n  {label} (probability: {prob: .3f})\\n')\n"
      ],
      "metadata": {
        "id": "lTXM_Yau-hOo"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_result(\"Just what I needed at this hour! a flat tire. Amazing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnMH0UgX6p-Q",
        "outputId": "f3706a1d-20dc-4131-f003-3c78d878cbf1"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Just what I needed at this hour! a flat tire. Amazing \n",
            "  Bazinga (probability:  0.829)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zo-h_dEN--S9"
      },
      "execution_count": 98,
      "outputs": []
    }
  ]
}